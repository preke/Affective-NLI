{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8997642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>labels</th>\n",
       "      <th>emotions</th>\n",
       "      <th>dialog_state</th>\n",
       "      <th>origin_sent</th>\n",
       "      <th>personality_description</th>\n",
       "      <th>affective_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Speaker : what a coincidence; Speaker : the ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>['neutral', 'neutral', 'neutral', 'neutral']</td>\n",
       "      <td>[1, 1, 0, 1]</td>\n",
       "      <td>Speaker : what a coincidence; Speaker : the ca...</td>\n",
       "      <td>Speaker is friendly, cooperative, empathetic, ...</td>\n",
       "      <td>The emotion of Speaker is initially neutral, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Speaker : right; Others : Your kids go to scho...</td>\n",
       "      <td>1</td>\n",
       "      <td>['neutral', 'neutral', 'neutral']</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>Speaker : right; Others : Your kids go to scho...</td>\n",
       "      <td>Speaker is friendly, cooperative, empathetic, ...</td>\n",
       "      <td>The emotion of Speaker is initially neutral, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaker : What a coincidence my son is here; S...</td>\n",
       "      <td>0</td>\n",
       "      <td>['neutral', 'neutral', 'neutral', 'neutral']</td>\n",
       "      <td>[1, 1, 0, 1]</td>\n",
       "      <td>Speaker : What a coincidence my son is here; S...</td>\n",
       "      <td>Speaker is friendly, cooperative, empathetic, ...</td>\n",
       "      <td>The emotion of Speaker is initially neutral, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Others : class three; Speaker : My son is also...</td>\n",
       "      <td>0</td>\n",
       "      <td>['neutral', 'neutral', 'neutral', 'neutral', '...</td>\n",
       "      <td>[0, 1, 1, 1, 0]</td>\n",
       "      <td>Others : class three; Speaker : My son is also...</td>\n",
       "      <td>Speaker is friendly, cooperative, empathetic, ...</td>\n",
       "      <td>First, the emotion of others is neutral, Speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Speaker : Ji Yangyang, it seems that I heard F...</td>\n",
       "      <td>0</td>\n",
       "      <td>['neutral', 'neutral', 'neutral', 'neutral', '...</td>\n",
       "      <td>[1, 1, 1, 0, 1]</td>\n",
       "      <td>Speaker : Ji Yangyang, it seems that I heard F...</td>\n",
       "      <td>Speaker is friendly, cooperative, empathetic, ...</td>\n",
       "      <td>The emotion of Speaker is initially neutral, S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent  labels  \\\n",
       "0  Speaker : what a coincidence; Speaker : the ca...       0   \n",
       "1  Speaker : right; Others : Your kids go to scho...       1   \n",
       "2  Speaker : What a coincidence my son is here; S...       0   \n",
       "3  Others : class three; Speaker : My son is also...       0   \n",
       "4  Speaker : Ji Yangyang, it seems that I heard F...       0   \n",
       "\n",
       "                                            emotions     dialog_state  \\\n",
       "0       ['neutral', 'neutral', 'neutral', 'neutral']     [1, 1, 0, 1]   \n",
       "1                  ['neutral', 'neutral', 'neutral']        [1, 0, 1]   \n",
       "2       ['neutral', 'neutral', 'neutral', 'neutral']     [1, 1, 0, 1]   \n",
       "3  ['neutral', 'neutral', 'neutral', 'neutral', '...  [0, 1, 1, 1, 0]   \n",
       "4  ['neutral', 'neutral', 'neutral', 'neutral', '...  [1, 1, 1, 0, 1]   \n",
       "\n",
       "                                         origin_sent  \\\n",
       "0  Speaker : what a coincidence; Speaker : the ca...   \n",
       "1  Speaker : right; Others : Your kids go to scho...   \n",
       "2  Speaker : What a coincidence my son is here; S...   \n",
       "3  Others : class three; Speaker : My son is also...   \n",
       "4  Speaker : Ji Yangyang, it seems that I heard F...   \n",
       "\n",
       "                             personality_description  \\\n",
       "0  Speaker is friendly, cooperative, empathetic, ...   \n",
       "1  Speaker is friendly, cooperative, empathetic, ...   \n",
       "2  Speaker is friendly, cooperative, empathetic, ...   \n",
       "3  Speaker is friendly, cooperative, empathetic, ...   \n",
       "4  Speaker is friendly, cooperative, empathetic, ...   \n",
       "\n",
       "                                    affective_prompt  \n",
       "0  The emotion of Speaker is initially neutral, S...  \n",
       "1  The emotion of Speaker is initially neutral, t...  \n",
       "2  The emotion of Speaker is initially neutral, S...  \n",
       "3  First, the emotion of others is neutral, Speak...  \n",
       "4  The emotion of Speaker is initially neutral, S...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "\n",
    "\n",
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict, Features, ClassLabel, Value\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainerCallback\n",
    "import evaluate\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "df_data = pd.read_csv( '../new_data/CPED_A_with_role_1.tsv', sep = '\\t')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0020fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/wenzhy/.cache/huggingface/datasets/json/default-c99757b34ec36bff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f417536c1348638c62244ca5c4a468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73287ac15bd40639180261b687abbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47f34d621bf4bc699bb67d338d5c52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/wenzhy/.cache/huggingface/datasets/json/default-c99757b34ec36bff/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1e8083335446a180466ecc8b03eb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sent_affective', 'personality_description', 'labels'],\n",
       "        num_rows: 10611\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sent_affective', 'personality_description', 'labels'],\n",
       "        num_rows: 1326\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sent_affective', 'personality_description', 'labels'],\n",
       "        num_rows: 1327\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(tsv_file):\n",
    "    df = pd.read_csv(tsv_file, sep='\\t')\n",
    "    df['sent_affective'] = df['origin_sent'] + ' ' + df['affective_prompt']\n",
    "    data_path = '../data/tmp.jsonl'\n",
    "    json_data = df[['sent_affective', 'personality_description', 'labels']].to_dict(orient=\"records\")\n",
    "    with open(data_path, 'w') as outfile:\n",
    "        for row in json_data:\n",
    "            json.dump(row, outfile)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "    class_names = ['no', 'yes']\n",
    "    features = Features({'sent_affective': Value('string'), 'personality_description': Value('string'), 'labels': ClassLabel(names=class_names)})\n",
    "    dataset_dict = load_dataset(\"json\", data_files=data_path, features=features)\n",
    "\n",
    "    tmp_dict = dataset_dict['train'].train_test_split(test_size=0.2, shuffle=True, seed=SEED)\n",
    "    train_dataset, remaining_dataset = tmp_dict['train'], tmp_dict['test']\n",
    "    tmp_dict = remaining_dataset.train_test_split(test_size=0.5, shuffle=True, seed=SEED)\n",
    "    valid_dataset, test_dataset = tmp_dict['train'], tmp_dict['test']\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'validation': valid_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    return dataset_dict\n",
    "\n",
    "tsv_file = '../new_data/CPED_A_with_role_1.tsv'\n",
    "dataset_dict = load_data(tsv_file)\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4832c536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sent_affective', 'personality_description', 'labels'],\n",
       "        num_rows: 10611\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sent_affective', 'personality_description', 'labels'],\n",
       "        num_rows: 1326\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sent_affective', 'personality_description', 'labels'],\n",
       "        num_rows: 1327\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548776d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"Speaker : you take this and sort it out; Others : what is this; Speaker : recording evidence; Speaker : Provided by Lou Xuan; Others : Lou Xuan; Speaker : That's Miss Xuan; Others : he is willing to testify; Speaker : Duancongxin found that it contained all his phone calls with Liu Jun in the past three years.; Others : he recorded it all; Speaker : he has the habit of recording; Others : such a terrible habit;  The emotion of Speaker is initially neutral, the emotion of others is neutral, Speaker respond with neutral, Speaker respond with neutral, the emotion of others is neutral, Speaker respond with neutral, the emotion of others is happy, Speaker respond with neutral, the emotion of others is happy, Speaker respond with neutral, the emotion of others is fear, \",\n",
      "  \"text_b\": \"Speaker is friendly, cooperative, empathetic, and compassionate, often prioritizing harmonious relationships and the well-being of others.\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    cnt = 0\n",
    "    for data in dataset_dict[split]:\n",
    "        input_example = InputExample(text_a = data['sent_affective'], text_b = data['personality_description'], label=int(data['labels']), guid=cnt)\n",
    "        dataset[split].append(input_example)\n",
    "        cnt += 1\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc28fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': \"Speaker : you take this and sort it out; Others : what is this; Speaker : recording evidence; Speaker : Provided by Lou Xuan; Others : Lou Xuan; Speaker : That's Miss Xuan; Others : he is willing to testify; Speaker : Duancongxin found that it contained all his phone calls with Liu Jun in the past three years.; Others : he recorded it all; Speaker : he has the habit of recording; Others : such a terrible habit;  The emotion of Speaker is initially neutral, the emotion of others is neutral, Speaker respond with neutral, Speaker respond with neutral, the emotion of others is neutral, Speaker respond with neutral, the emotion of others is happy, Speaker respond with neutral, the emotion of others is happy, Speaker respond with neutral, the emotion of others is fear, \", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' Question:', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' Speaker is friendly, cooperative, empathetic, and compassionate, often prioritizing harmonious relationships and the well-being of others.', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '? Is it correct?', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'label': 0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")\n",
    "\n",
    "from openprompt.prompts import ManualTemplate\n",
    "template_text = '{\"placeholder\":\"text_a\"} Question: {\"placeholder\":\"text_b\"}? Is it correct? {\"mask\"}.'\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
    "\n",
    "# To better understand how does the template wrap the example, we visualize one instance.\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238955b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [16778, 3, 10, 25, 240, 48, 11, 1843, 34, 91, 117, 14818, 3, 10, 125, 19, 48, 117, 16778, 3, 10, 5592, 2084, 117, 16778, 3, 10, 7740, 26, 57, 11884, 3, 4, 76, 152, 117, 14818, 3, 10, 11884, 3, 4, 76, 152, 117, 16778, 3, 10, 466, 31, 7, 5964, 3, 4, 76, 152, 117, 14818, 3, 10, 3, 88, 19, 4403, 12, 794, 4921, 117, 16778, 3, 10, 970, 152, 1018, 122, 226, 77, 435, 24, 34, 6966, 66, 112, 951, 3088, 28, 1414, 76, 10745, 16, 8, 657, 386, 203, 5, 117, 14818, 3, 10, 3, 88, 4381, 34, 66, 117, 16778, 3, 10, 3, 88, 65, 8, 7386, 13, 5592, 117, 14818, 3, 10, 224, 3, 9, 9412, 7386, 117, 37, 13868, 13, 16778, 19, 7513, 7163, 6, 8, 13868, 13, 717, 19, 7163, 6, 16778, 3531, 28, 7163, 6, 16778, 3531, 28, 7163, 6, 8, 13868, 13, 717, 19, 7163, 6, 16778, 3531, 28, 7163, 6, 8, 13868, 13, 717, 19, 1095, 6, 16778, 3531, 28, 7163, 6, 8, 13868, 13, 717, 19, 1095, 6, 16778, 3531, 28, 7163, 6, 8, 13868, 13, 717, 19, 2971, 6, 11860, 10, 16778, 19, 2609, 6, 20270, 6, 3, 15, 51, 27826, 6, 11, 21801, 6, 557, 1884, 155, 2610, 29938, 3079, 11, 8, 168, 18, 9032, 13, 717, 5, 3, 58, 27, 7, 34, 2024, 58, 32099, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'decoder_input_ids': [0, 32099, 0], 'loss_ids': [0, 1, 0]}\n",
      "['▁Speaker', '▁', ':', '▁you', '▁take', '▁this', '▁and', '▁sort', '▁it', '▁out', ';', '▁Others', '▁', ':', '▁what', '▁is', '▁this', ';', '▁Speaker', '▁', ':', '▁recording', '▁evidence', ';', '▁Speaker', '▁', ':', '▁Provide', 'd', '▁by', '▁Lou', '▁', 'X', 'u', 'an', ';', '▁Others', '▁', ':', '▁Lou', '▁', 'X', 'u', 'an', ';', '▁Speaker', '▁', ':', '▁That', \"'\", 's', '▁Miss', '▁', 'X', 'u', 'an', ';', '▁Others', '▁', ':', '▁', 'he', '▁is', '▁willing', '▁to', '▁test', 'ify', ';', '▁Speaker', '▁', ':', '▁Du', 'an', 'con', 'g', 'x', 'in', '▁found', '▁that', '▁it', '▁contained', '▁all', '▁his', '▁phone', '▁calls', '▁with', '▁Li', 'u', '▁Jun', '▁in', '▁the', '▁past', '▁three', '▁years', '.', ';', '▁Others', '▁', ':', '▁', 'he', '▁recorded', '▁it', '▁all', ';', '▁Speaker', '▁', ':', '▁', 'he', '▁has', '▁the', '▁habit', '▁of', '▁recording', ';', '▁Others', '▁', ':', '▁such', '▁', 'a', '▁terrible', '▁habit', ';', '▁The', '▁emotion', '▁of', '▁Speaker', '▁is', '▁initially', '▁neutral', ',', '▁the', '▁emotion', '▁of', '▁others', '▁is', '▁neutral', ',', '▁Speaker', '▁respond', '▁with', '▁neutral', ',', '▁Speaker', '▁respond', '▁with', '▁neutral', ',', '▁the', '▁emotion', '▁of', '▁others', '▁is', '▁neutral', ',', '▁Speaker', '▁respond', '▁with', '▁neutral', ',', '▁the', '▁emotion', '▁of', '▁others', '▁is', '▁happy', ',', '▁Speaker', '▁respond', '▁with', '▁neutral', ',', '▁the', '▁emotion', '▁of', '▁others', '▁is', '▁happy', ',', '▁Speaker', '▁respond', '▁with', '▁neutral', ',', '▁the', '▁emotion', '▁of', '▁others', '▁is', '▁fear', ',', '▁Question', ':', '▁Speaker', '▁is', '▁friendly', ',', '▁cooperative', ',', '▁', 'e', 'm', 'pathetic', ',', '▁and', '▁compassionate', ',', '▁often', '▁prior', 'it', 'izing', '▁harmonious', '▁relationships', '▁and', '▁the', '▁well', '-', 'being', '▁of', '▁others', '.', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<pad>', '<extra_id_0>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import T5TokenizerWrapper\n",
    "wrapped_t5tokenizer= T5TokenizerWrapper(max_seq_length=256, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "\n",
    "# You can see what a tokenized example looks like by\n",
    "tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)\n",
    "print(tokenized_example)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_example['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_example['decoder_input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44a3267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "model_inputs = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    model_inputs[split] = []\n",
    "    for sample in dataset[split]:\n",
    "        tokenized_example = wrapped_t5tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)\n",
    "        model_inputs[split].append(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83ac393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 10611it [00:16, 631.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "# next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ab9e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 150]],\n",
      "\n",
      "        [[4273]]])\n",
      "tensor([[-0.9410, -0.4947],\n",
      "        [-0.5593, -0.8477]])\n"
     ]
    }
   ],
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=2,\n",
    "                        label_words=[[\"no\"], [\"yes\"]])\n",
    "\n",
    "print(myverbalizer.label_words_ids)\n",
    "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and\n",
    "print(myverbalizer.process_logits(logits)) # see what the verbalizer do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42c99918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a83e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenzhy/wenzhy/ENTER/envs/wen/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 1.7040561437606812\n",
      "Epoch 0, average loss: 0.7777448433871362\n",
      "Epoch 0, average loss: 0.7493293055951005\n",
      "Epoch 0, average loss: 0.7307587437183651\n",
      "Epoch 0, average loss: 0.7258040940954318\n",
      "Epoch 0, average loss: 0.7202348490338402\n",
      "Epoch 0, average loss: 0.714383659147939\n",
      "Epoch 0, average loss: 0.7114625106807109\n",
      "Epoch 0, average loss: 0.709003147731844\n",
      "Epoch 0, average loss: 0.7047489849962046\n",
      "Epoch 0, average loss: 0.7047207327094859\n",
      "Epoch 0, average loss: 0.7019664605575767\n",
      "Epoch 0, average loss: 0.6974901690732025\n",
      "Epoch 0, average loss: 0.6972014470835618\n",
      "Epoch 0, average loss: 0.6967823299102368\n",
      "Epoch 0, average loss: 0.6961770435167692\n",
      "Epoch 0, average loss: 0.6938605670085858\n",
      "Epoch 0, average loss: 0.6914871523001359\n",
      "Epoch 0, average loss: 0.6909613525877385\n",
      "Epoch 0, average loss: 0.6891216582586337\n",
      "Epoch 0, average loss: 0.687702362130691\n",
      "Epoch 0, average loss: 0.6867435375471324\n",
      "Epoch 0, average loss: 0.6858001969734394\n",
      "Epoch 1, average loss: 0.6115846768120267\n",
      "Epoch 1, average loss: 0.6050151930704936\n",
      "Epoch 1, average loss: 0.6065807739250451\n",
      "Epoch 1, average loss: 0.6085177319019142\n",
      "Epoch 1, average loss: 0.6050605119824239\n",
      "Epoch 1, average loss: 0.6034666424770456\n",
      "Epoch 1, average loss: 0.6002473930214963\n",
      "Epoch 1, average loss: 0.600417093119162\n",
      "Epoch 1, average loss: 0.5998988353365021\n",
      "Epoch 1, average loss: 0.5992850615223018\n",
      "Epoch 1, average loss: 0.600645820419955\n",
      "Epoch 1, average loss: 0.6012289227702563\n",
      "Epoch 1, average loss: 0.5999190657289225\n",
      "Epoch 1, average loss: 0.6017054504623053\n",
      "Epoch 1, average loss: 0.6049168124908776\n",
      "Epoch 1, average loss: 0.6048258593746282\n",
      "Epoch 1, average loss: 0.6045697752061201\n",
      "Epoch 1, average loss: 0.6052285744906305\n",
      "Epoch 1, average loss: 0.6047750191530877\n",
      "Epoch 1, average loss: 0.6031636299394565\n",
      "Epoch 1, average loss: 0.6040288386955456\n",
      "Epoch 1, average loss: 0.6039827266315636\n",
      "Epoch 1, average loss: 0.6033416586421091\n",
      "Epoch 1, average loss: 0.6026704802330323\n",
      "Epoch 2, average loss: 0.30954839289188385\n",
      "Epoch 2, average loss: 0.405147253298292\n",
      "Epoch 2, average loss: 0.3955539513176473\n",
      "Epoch 2, average loss: 0.37935458831119023\n",
      "Epoch 2, average loss: 0.38485514064463067\n",
      "Epoch 2, average loss: 0.37894873642381205\n",
      "Epoch 2, average loss: 0.38182809679734947\n",
      "Epoch 2, average loss: 0.3869673214385822\n",
      "Epoch 2, average loss: 0.386794026894611\n",
      "Epoch 2, average loss: 0.3885152015139136\n",
      "Epoch 2, average loss: 0.38866378935035356\n",
      "Epoch 2, average loss: 0.3864078671551443\n",
      "Epoch 2, average loss: 0.3920358325976153\n",
      "Epoch 2, average loss: 0.38872430762833987\n",
      "Epoch 2, average loss: 0.3920925028340495\n",
      "Epoch 2, average loss: 0.3938381770568665\n",
      "Epoch 2, average loss: 0.39531781010971534\n",
      "Epoch 2, average loss: 0.3957265945705208\n",
      "Epoch 2, average loss: 0.39695796345905315\n",
      "Epoch 2, average loss: 0.39899602544334345\n",
      "Epoch 2, average loss: 0.39841251830560165\n",
      "Epoch 2, average loss: 0.39902364732440626\n",
      "Epoch 2, average loss: 0.39900221564400123\n",
      "Epoch 2, average loss: 0.4007161271893504\n",
      "Epoch 2, average loss: 0.40169907728912657\n",
      "Epoch 2, average loss: 0.40263048682412467\n",
      "Epoch 2, average loss: 0.4020312489264521\n",
      "Epoch 3, average loss: 0.2979137599468231\n",
      "Epoch 3, average loss: 0.1496709578707084\n",
      "Epoch 3, average loss: 0.13938355741107208\n",
      "Epoch 3, average loss: 0.1350440419965581\n",
      "Epoch 3, average loss: 0.13564436281523007\n",
      "Epoch 3, average loss: 0.14119421436414967\n",
      "Epoch 3, average loss: 0.1530067395367443\n",
      "Epoch 3, average loss: 0.14936097646234928\n",
      "Epoch 3, average loss: 0.1475118577413617\n",
      "Epoch 3, average loss: 0.1453265269848228\n",
      "Epoch 3, average loss: 0.14609239547205075\n",
      "Epoch 3, average loss: 0.1486605688939171\n",
      "Epoch 3, average loss: 0.15067938495333036\n",
      "Epoch 3, average loss: 0.15315297252868557\n",
      "Epoch 3, average loss: 0.15416207182432523\n",
      "Epoch 3, average loss: 0.1589231101688529\n",
      "Epoch 3, average loss: 0.16125337249799526\n",
      "Epoch 3, average loss: 0.16255802624412544\n",
      "Epoch 3, average loss: 0.16516920614003186\n",
      "Epoch 3, average loss: 0.1689577235990646\n",
      "Epoch 3, average loss: 0.17021830686588202\n",
      "Epoch 3, average loss: 0.17274239172466824\n",
      "Epoch 3, average loss: 0.17331820433968564\n",
      "Epoch 3, average loss: 0.1744583166546205\n",
      "Epoch 3, average loss: 0.17514462623267618\n",
      "Epoch 3, average loss: 0.17524420334760724\n",
      "Epoch 3, average loss: 0.17617015334588998\n",
      "Epoch 4, average loss: 0.06742339301854372\n",
      "Epoch 4, average loss: 0.0973715749729023\n",
      "Epoch 4, average loss: 0.07698449842638805\n",
      "Epoch 4, average loss: 0.07682581368880953\n",
      "Epoch 4, average loss: 0.07553948530047778\n",
      "Epoch 4, average loss: 0.07568885557806374\n",
      "Epoch 4, average loss: 0.07527229261582621\n",
      "Epoch 4, average loss: 0.07418370022437389\n",
      "Epoch 4, average loss: 0.07544116898454485\n",
      "Epoch 4, average loss: 0.07731468885631854\n",
      "Epoch 4, average loss: 0.07815830041562333\n",
      "Epoch 4, average loss: 0.08090662365143486\n",
      "Epoch 4, average loss: 0.0831070528262646\n",
      "Epoch 4, average loss: 0.08183244282198954\n",
      "Epoch 4, average loss: 0.08429843586577752\n",
      "Epoch 4, average loss: 0.0831938418077056\n",
      "Epoch 4, average loss: 0.08385596574850426\n",
      "Epoch 4, average loss: 0.08376304763246871\n",
      "Epoch 4, average loss: 0.08558926034349455\n",
      "Epoch 4, average loss: 0.08484686443576293\n",
      "Epoch 4, average loss: 0.08405681872262959\n",
      "Epoch 4, average loss: 0.08462104312063866\n",
      "Epoch 4, average loss: 0.08505989240884042\n",
      "Epoch 4, average loss: 0.08693041274391022\n",
      "Epoch 4, average loss: 0.08758795095530147\n",
      "Epoch 4, average loss: 0.0895596421463754\n",
      "Epoch 4, average loss: 0.09063248340578804\n",
      "Epoch 5, average loss: 0.05090201832354069\n",
      "Epoch 5, average loss: 0.05331499472759235\n",
      "Epoch 5, average loss: 0.05204551781301207\n",
      "Epoch 5, average loss: 0.06131252441736438\n",
      "Epoch 5, average loss: 0.05326506555127762\n",
      "Epoch 5, average loss: 0.0521205648567185\n",
      "Epoch 5, average loss: 0.04795455771733618\n",
      "Epoch 5, average loss: 0.04623956126061575\n",
      "Epoch 5, average loss: 0.050302223792459584\n",
      "Epoch 5, average loss: 0.04945647493224479\n",
      "Epoch 5, average loss: 0.05115794816017746\n",
      "Epoch 5, average loss: 0.050344376410603485\n",
      "Epoch 5, average loss: 0.05303239574079185\n",
      "Epoch 5, average loss: 0.05464478787469582\n",
      "Epoch 5, average loss: 0.057587310697663034\n",
      "Epoch 5, average loss: 0.057674654079626325\n",
      "Epoch 5, average loss: 0.06026190814459403\n",
      "Epoch 5, average loss: 0.05979758325254103\n",
      "Epoch 5, average loss: 0.060667414482212835\n",
      "Epoch 5, average loss: 0.06403930544130124\n",
      "Epoch 5, average loss: 0.06357202214427674\n",
      "Epoch 5, average loss: 0.06297462700996972\n",
      "Epoch 5, average loss: 0.06426466172639368\n",
      "Epoch 5, average loss: 0.06541917800877088\n",
      "Epoch 5, average loss: 0.06532272196178196\n",
      "Epoch 5, average loss: 0.06629376398040732\n",
      "Epoch 5, average loss: 0.06707136615567905\n",
      "Epoch 6, average loss: 0.08388329902663827\n",
      "Epoch 6, average loss: 0.03739997039823114\n",
      "Epoch 6, average loss: 0.03985153016370838\n",
      "Epoch 6, average loss: 0.031292395264136574\n",
      "Epoch 6, average loss: 0.027241025126255154\n",
      "Epoch 6, average loss: 0.030652215405304644\n",
      "Epoch 6, average loss: 0.027560860575797194\n",
      "Epoch 6, average loss: 0.027492009049264226\n",
      "Epoch 6, average loss: 0.02717550530058802\n",
      "Epoch 6, average loss: 0.027244351377332998\n",
      "Epoch 6, average loss: 0.02679357123841833\n",
      "Epoch 6, average loss: 0.028322329879025766\n",
      "Epoch 6, average loss: 0.028586226928073437\n",
      "Epoch 6, average loss: 0.02995110434245797\n",
      "Epoch 6, average loss: 0.03087651241911173\n",
      "Epoch 6, average loss: 0.03079069424452712\n",
      "Epoch 6, average loss: 0.032471238298027214\n",
      "Epoch 6, average loss: 0.03351621531795029\n",
      "Epoch 6, average loss: 0.03328443659281344\n",
      "Epoch 6, average loss: 0.033731424015690695\n",
      "Epoch 6, average loss: 0.03398560047303949\n",
      "Epoch 6, average loss: 0.035239864374553426\n",
      "Epoch 6, average loss: 0.03474272451007715\n",
      "Epoch 6, average loss: 0.03591702683695584\n",
      "Epoch 6, average loss: 0.03625983978401753\n",
      "Epoch 6, average loss: 0.03590220920602074\n",
      "Epoch 6, average loss: 0.036641390741192124\n",
      "Epoch 7, average loss: 0.01585178286768496\n",
      "Epoch 7, average loss: 0.058406609339575276\n",
      "Epoch 7, average loss: 0.04008487166004328\n",
      "Epoch 7, average loss: 0.030525900675503474\n",
      "Epoch 7, average loss: 0.03428165615707316\n",
      "Epoch 7, average loss: 0.03207675072510836\n",
      "Epoch 7, average loss: 0.029293914418934513\n",
      "Epoch 7, average loss: 0.02930353326300851\n",
      "Epoch 7, average loss: 0.02826117584504091\n",
      "Epoch 7, average loss: 0.030104896827803325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, average loss: 0.029356451914799082\n",
      "Epoch 7, average loss: 0.02800854023282915\n",
      "Epoch 7, average loss: 0.029577363353567775\n",
      "Epoch 7, average loss: 0.0322625037812614\n",
      "Epoch 7, average loss: 0.03274802302734844\n",
      "Epoch 7, average loss: 0.03226741234982478\n",
      "Epoch 7, average loss: 0.03132250087921733\n",
      "Epoch 7, average loss: 0.0308871243525511\n",
      "Epoch 7, average loss: 0.0299997952374312\n",
      "Epoch 7, average loss: 0.030624240568078647\n",
      "Epoch 7, average loss: 0.030961175842163374\n",
      "Epoch 7, average loss: 0.03282383960143831\n",
      "Epoch 7, average loss: 0.032659994309072626\n",
      "Epoch 7, average loss: 0.032583309999057036\n",
      "Epoch 7, average loss: 0.03321843351294593\n",
      "Epoch 7, average loss: 0.033089033917180696\n",
      "Epoch 7, average loss: 0.03378937547852216\n",
      "Epoch 8, average loss: 0.00011076860755565576\n",
      "Epoch 8, average loss: 0.01106238060236039\n",
      "Epoch 8, average loss: 0.023683162036833197\n",
      "Epoch 8, average loss: 0.020166970134245493\n",
      "Epoch 8, average loss: 0.017803861196949614\n",
      "Epoch 8, average loss: 0.018099096255366202\n",
      "Epoch 8, average loss: 0.01566068393271045\n",
      "Epoch 8, average loss: 0.01598760627253185\n",
      "Epoch 8, average loss: 0.016716462707285447\n",
      "Epoch 8, average loss: 0.016179391096498366\n",
      "Epoch 8, average loss: 0.01545317235707427\n",
      "Epoch 8, average loss: 0.01499235908472294\n",
      "Epoch 8, average loss: 0.01561885812885426\n",
      "Epoch 8, average loss: 0.018643590154570105\n",
      "Epoch 8, average loss: 0.021125131152088365\n",
      "Epoch 8, average loss: 0.024201984272861913\n",
      "Epoch 8, average loss: 0.024818129869629068\n",
      "Epoch 8, average loss: 0.025387793008367396\n",
      "Epoch 8, average loss: 0.02526118714223942\n",
      "Epoch 8, average loss: 0.024707366862812438\n",
      "Epoch 8, average loss: 0.025696293260946693\n",
      "Epoch 8, average loss: 0.031019091945631846\n",
      "Epoch 8, average loss: 0.031636830173591465\n",
      "Epoch 8, average loss: 0.033579307566534135\n",
      "Epoch 8, average loss: 0.033143698523603694\n",
      "Epoch 8, average loss: 0.03345936536889076\n",
      "Epoch 8, average loss: 0.034433192148891005\n",
      "Epoch 9, average loss: 0.0023956423901836388\n",
      "Epoch 9, average loss: 0.026119383480075906\n",
      "Epoch 9, average loss: 0.02167203815225764\n",
      "Epoch 9, average loss: 0.018875219049709964\n",
      "Epoch 9, average loss: 0.018074242784119243\n",
      "Epoch 9, average loss: 0.022715716391122256\n",
      "Epoch 9, average loss: 0.02463061869210292\n",
      "Epoch 9, average loss: 0.027077359224110743\n",
      "Epoch 9, average loss: 0.02555821818050855\n",
      "Epoch 9, average loss: 0.025032386276667176\n",
      "Epoch 9, average loss: 0.028996736848437513\n",
      "Epoch 9, average loss: 0.031100097766951728\n",
      "Epoch 9, average loss: 0.030359872219218414\n",
      "Epoch 9, average loss: 0.03478690676033064\n",
      "Epoch 9, average loss: 0.033753034649699754\n",
      "Epoch 9, average loss: 0.0348405035845221\n",
      "Epoch 9, average loss: 0.03520765371865738\n",
      "Epoch 9, average loss: 0.03406363324245232\n",
      "Epoch 9, average loss: 0.03646486827833742\n",
      "Epoch 9, average loss: 0.03757185961333591\n",
      "Epoch 9, average loss: 0.03756889110617244\n",
      "Epoch 9, average loss: 0.03760579631292994\n",
      "Epoch 9, average loss: 0.037490208664923745\n",
      "Epoch 9, average loss: 0.03680753170382612\n",
      "Epoch 9, average loss: 0.03647992939934053\n",
      "Epoch 9, average loss: 0.03714801302864486\n",
      "Epoch 9, average loss: 0.037474589951686704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1326it [00:01, 754.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5950226244343891\n"
     ]
    }
   ],
   "source": [
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step %100 ==1:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n",
    "\n",
    "# Evaluate\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wen",
   "language": "python",
   "name": "wen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
