{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7991903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_data = pd.read_csv('../data/Friends_A_whole.tsv', sep = '\\t')\n",
    "df = df_data[['utterance', 'labels']]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Uttr_train, Uttr_test, label_train, label_test = \\\n",
    "    train_test_split(df['utterance'], df['labels'], test_size=0.1, random_state=42, stratify=df['labels'])\n",
    "\n",
    "Uttr_train, Uttr_valid, label_train, label_valid = \\\n",
    "    train_test_split(Uttr_train, label_train, test_size=0.1, random_state=42, stratify=label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d31380",
   "metadata": {},
   "source": [
    "## 构建template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b058394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"  Okay, (reading the card) Fonzy gives you two thumbs up, collect two cool points. Yeah.  Okay, come on! (blows on the dice) Daddy needs a new pair of electromagnetic microscopes for the Prehistoric Forensics Department! (They all look at him, and he shuts up and rolls the dice.) (he moves his piece) Okay. (reading a card) Take Pinky Tuscadero up to Inspiration Point, collect three cool points!! Yeah! Which gives me five, and let's see who is gonna lose their clothes. Ummmm, I think I pick our strip poker sponsor Mr. Joey Tribianni.\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "classes = [ \n",
    "    \"Yes\",\n",
    "    \"No\"\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    cnt = 0\n",
    "    for u,l in zip(Uttr_train, label_train):\n",
    "        input_sample = InputExample(text_a=u, label=int(l),guid=cnt)\n",
    "        cnt += 1\n",
    "        dataset[split].append(input_sample)\n",
    "        \n",
    "        \n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030487b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f77b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': \"  Okay, (reading the card) Fonzy gives you two thumbs up, collect two cool points. Yeah.  Okay, come on! (blows on the dice) Daddy needs a new pair of electromagnetic microscopes for the Prehistoric Forensics Department! (They all look at him, and he shuts up and rolls the dice.) (he moves his piece) Okay. (reading a card) Take Pinky Tuscadero up to Inspiration Point, collect three cool points!! Yeah! Which gives me five, and let's see who is gonna lose their clothes. Ummmm, I think I pick our strip poker sponsor Mr. Joey Tribianni.\", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' He is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 0, 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "mytemplate = ManualTemplate(\n",
    "    text = '{\"placeholder\":\"text_a\"} He is {\"mask\"}',\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "150a46fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BERTTokenizerWrapper' from 'openprompt.plms' (/home/zhiyuan/ENTER/lib/python3.9/site-packages/openprompt/plms/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21296/2615382506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwrapped_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWrapperClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncate_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"head\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mopenprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBERTTokenizerWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwrapped_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mBERTTokenizerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncate_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"head\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BERTTokenizerWrapper' from 'openprompt.plms' (/home/zhiyuan/ENTER/lib/python3.9/site-packages/openprompt/plms/__init__.py)"
     ]
    }
   ],
   "source": [
    "wrapped_tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "# or\n",
    "from openprompt.plms import BERTTokenizerWrapper\n",
    "wrapped_tokenizer = BERTTokenizerWrapper(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b660a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "# next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd154cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[1185]],\n",
      "\n",
      "        [[4208]]])\n",
      "tensor([[-0.7534, -0.6363],\n",
      "        [-0.6803, -0.7062]])\n"
     ]
    }
   ],
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=2,\n",
    "                        label_words=[[\"no\"], [\"yes\"]])\n",
    "\n",
    "print(myverbalizer.label_words_ids)\n",
    "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and\n",
    "print(myverbalizer.process_logits(logits)) # see what the verbalizer do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
