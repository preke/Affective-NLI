{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b031dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_data = pd.read_csv('../data/Friends_A_whole.tsv', sep = '\\t')\n",
    "df = df_data[['utterance', 'labels']]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Uttr_train, Uttr_test, label_train, label_test = \\\n",
    "    train_test_split(df['utterance'], df['labels'], test_size=0.1, random_state=42, stratify=df['labels'])\n",
    "\n",
    "Uttr_train, Uttr_valid, label_train, label_valid = \\\n",
    "    train_test_split(Uttr_train, label_train, test_size=0.1, random_state=42, stratify=label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78cb8c",
   "metadata": {},
   "source": [
    "## 构建template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7ddc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"  Okay, (reading the card) Fonzy gives you two thumbs up, collect two cool points. Yeah.  Okay, come on! (blows on the dice) Daddy needs a new pair of electromagnetic microscopes for the Prehistoric Forensics Department! (They all look at him, and he shuts up and rolls the dice.) (he moves his piece) Okay. (reading a card) Take Pinky Tuscadero up to Inspiration Point, collect three cool points!! Yeah! Which gives me five, and let's see who is gonna lose their clothes. Ummmm, I think I pick our strip poker sponsor Mr. Joey Tribianni.\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    " \n",
    "\n",
    "dataset = {}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    cnt = 0\n",
    "    for u,l in zip(Uttr_train, label_train):\n",
    "        input_sample = InputExample(text_a=u, label=int(l),guid=cnt)\n",
    "        cnt += 1\n",
    "        dataset[split].append(input_sample)\n",
    "        \n",
    "        \n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bcec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5843b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': \"  Okay, (reading the card) Fonzy gives you two thumbs up, collect two cool points. Yeah.  Okay, come on! (blows on the dice) Daddy needs a new pair of electromagnetic microscopes for the Prehistoric Forensics Department! (They all look at him, and he shuts up and rolls the dice.) (he moves his piece) Okay. (reading a card) Take Pinky Tuscadero up to Inspiration Point, collect three cool points!! Yeah! Which gives me five, and let's see who is gonna lose their clothes. Ummmm, I think I pick our strip poker sponsor Mr. Joey Tribianni.\", 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' He is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 0, 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "mytemplate = ManualTemplate(\n",
    "    text = '{\"placeholder\":\"text_a\"} He is {\"mask\"}',\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63651a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10044f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3956, 117, 113, 3455, 1103, 3621, 114, 143, 1320, 6482, 3114, 1128, 1160, 18680, 1146, 117, 7822, 1160, 4348, 1827, 119, 2814, 119, 3956, 117, 1435, 1113, 106, 113, 14977, 1113, 1103, 26104, 114, 9979, 2993, 170, 1207, 3111, 1104, 19805, 17599, 15300, 1116, 1111, 1103, 11689, 27516, 2772, 1596, 1370, 5026, 4724, 1951, 106, 113, 1220, 1155, 1440, 1120, 1140, 117, 1105, 1119, 3210, 1116, 1146, 1105, 12205, 1103, 26104, 119, 114, 113, 1119, 5279, 1117, 2727, 114, 3956, 119, 113, 3455, 170, 3621, 114, 5055, 10763, 1183, 17037, 26996, 2692, 1186, 1146, 1106, 1130, 21240, 4221, 117, 7822, 1210, 4348, 1827, 106, 106, 2814, 106, 5979, 3114, 1143, 1421, 117, 1105, 1519, 112, 188, 1267, 1150, 1110, 6100, 3857, 1147, 3459, 1124, 1110, 103, 102], 'loss_ids': [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, -100], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'Okay', ',', '(', 'reading', 'the', 'card', ')', 'F', '##on', '##zy', 'gives', 'you', 'two', 'thumbs', 'up', ',', 'collect', 'two', 'cool', 'points', '.', 'Yeah', '.', 'Okay', ',', 'come', 'on', '!', '(', 'blows', 'on', 'the', 'dice', ')', 'Daddy', 'needs', 'a', 'new', 'pair', 'of', 'electromagnetic', 'micro', '##scope', '##s', 'for', 'the', 'Pre', '##his', '##tor', '##ic', 'For', '##ens', '##ics', 'Department', '!', '(', 'They', 'all', 'look', 'at', 'him', ',', 'and', 'he', 'shut', '##s', 'up', 'and', 'rolls', 'the', 'dice', '.', ')', '(', 'he', 'moves', 'his', 'piece', ')', 'Okay', '.', '(', 'reading', 'a', 'card', ')', 'Take', 'Pink', '##y', 'Tu', '##sca', '##der', '##o', 'up', 'to', 'In', '##spiration', 'Point', ',', 'collect', 'three', 'cool', 'points', '!', '!', 'Yeah', '!', 'Which', 'gives', 'me', 'five', ',', 'and', 'let', \"'\", 's', 'see', 'who', 'is', 'gonna', 'lose', 'their', 'clothes', 'He', 'is', '[MASK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = wrapped_tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)\n",
    "print(tokenized_example)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_example['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdaacf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "model_inputs = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    model_inputs[split] = []\n",
    "    for sample in dataset[split]:\n",
    "        tokenized_example = wrapped_tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)\n",
    "        model_inputs[split].append(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f14efcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 575it [00:00, 1759.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "# next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fad6fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[23423,  1895]],\n",
      "\n",
      "        [[ 5340,  1895]]])\n",
      "tensor([[-0.4659, -0.9877],\n",
      "        [-0.2609, -1.4712]])\n"
     ]
    }
   ],
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=2,\n",
    "                        label_words=[[\"disagreeable\"], [\"agreeable\"]])\n",
    "\n",
    "print(myverbalizer.label_words_ids)\n",
    "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and\n",
    "print(myverbalizer.process_logits(logits)) # see what the verbalizer do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b699e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9eb03cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 4.126529097557068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhiyuan/ENTER/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 0.8543983506513577\n",
      "Epoch 1, average loss: 0.6764234602451324\n",
      "Epoch 1, average loss: 0.6893489284842622\n",
      "Epoch 2, average loss: 0.6995877027511597\n",
      "Epoch 2, average loss: 0.6833851664674049\n",
      "Epoch 3, average loss: 0.752496600151062\n",
      "Epoch 3, average loss: 0.7129354307464525\n",
      "Epoch 4, average loss: 0.7239862978458405\n",
      "Epoch 4, average loss: 0.6950551268516802\n",
      "Epoch 5, average loss: 0.7413607835769653\n",
      "Epoch 5, average loss: 0.7062824911930982\n",
      "Epoch 6, average loss: 0.7496736645698547\n",
      "Epoch 6, average loss: 0.6900721946183372\n",
      "Epoch 7, average loss: 0.6635996699333191\n",
      "Epoch 7, average loss: 0.6916192197332195\n",
      "Epoch 8, average loss: 0.620771199464798\n",
      "Epoch 8, average loss: 0.6913934899311439\n",
      "Epoch 9, average loss: 0.5646192729473114\n",
      "Epoch 9, average loss: 0.6917744246768016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 575it [00:00, 1748.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5704347826086956\n"
     ]
    }
   ],
   "source": [
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step %100 ==1:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n",
    "\n",
    "# Evaluate\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
