{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "tensor([[1]])\n",
      "tensor([[-0.0996, -0.8046]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "from transformers import BertConfig, BertModel, BertPreTrainedModel\n",
    "import simpletransformers\n",
    "import torch\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "inputs =[tokenizer.encode(sent, add_special_tokens=True, max_length=64, \\\n",
    "                pad_to_max_length=True) for sent in [\"Hello, my dog is cute\"]]\n",
    "\n",
    "inputs = torch.tensor(inputs)\n",
    "print(inputs)\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "print(labels)\n",
    "outputs = model(inputs, labels=labels)\n",
    "logits = outputs.logits\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing DialogVAD: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DialogVAD from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DialogVAD from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DialogVAD were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['get_vad.weight', 'get_vad.bias', 'context_interaction.layers.0.self_attn.in_proj_weight', 'context_interaction.layers.0.self_attn.in_proj_bias', 'context_interaction.layers.0.self_attn.out_proj.weight', 'context_interaction.layers.0.self_attn.out_proj.bias', 'context_interaction.layers.0.linear1.weight', 'context_interaction.layers.0.linear1.bias', 'context_interaction.layers.0.linear2.weight', 'context_interaction.layers.0.linear2.bias', 'context_interaction.layers.0.norm1.weight', 'context_interaction.layers.0.norm1.bias', 'context_interaction.layers.0.norm2.weight', 'context_interaction.layers.0.norm2.bias', 'context_interaction.layers.1.self_attn.in_proj_weight', 'context_interaction.layers.1.self_attn.in_proj_bias', 'context_interaction.layers.1.self_attn.out_proj.weight', 'context_interaction.layers.1.self_attn.out_proj.bias', 'context_interaction.layers.1.linear1.weight', 'context_interaction.layers.1.linear1.bias', 'context_interaction.layers.1.linear2.weight', 'context_interaction.layers.1.linear2.bias', 'context_interaction.layers.1.norm1.weight', 'context_interaction.layers.1.norm1.bias', 'context_interaction.layers.1.norm2.weight', 'context_interaction.layers.1.norm2.bias', 'context_interaction.layers.2.self_attn.in_proj_weight', 'context_interaction.layers.2.self_attn.in_proj_bias', 'context_interaction.layers.2.self_attn.out_proj.weight', 'context_interaction.layers.2.self_attn.out_proj.bias', 'context_interaction.layers.2.linear1.weight', 'context_interaction.layers.2.linear1.bias', 'context_interaction.layers.2.linear2.weight', 'context_interaction.layers.2.linear2.bias', 'context_interaction.layers.2.norm1.weight', 'context_interaction.layers.2.norm1.bias', 'context_interaction.layers.2.norm2.weight', 'context_interaction.layers.2.norm2.bias', 'context_interaction.layers.3.self_attn.in_proj_weight', 'context_interaction.layers.3.self_attn.in_proj_bias', 'context_interaction.layers.3.self_attn.out_proj.weight', 'context_interaction.layers.3.self_attn.out_proj.bias', 'context_interaction.layers.3.linear1.weight', 'context_interaction.layers.3.linear1.bias', 'context_interaction.layers.3.linear2.weight', 'context_interaction.layers.3.linear2.bias', 'context_interaction.layers.3.norm1.weight', 'context_interaction.layers.3.norm1.bias', 'context_interaction.layers.3.norm2.weight', 'context_interaction.layers.3.norm2.bias', 'context_interaction.layers.4.self_attn.in_proj_weight', 'context_interaction.layers.4.self_attn.in_proj_bias', 'context_interaction.layers.4.self_attn.out_proj.weight', 'context_interaction.layers.4.self_attn.out_proj.bias', 'context_interaction.layers.4.linear1.weight', 'context_interaction.layers.4.linear1.bias', 'context_interaction.layers.4.linear2.weight', 'context_interaction.layers.4.linear2.bias', 'context_interaction.layers.4.norm1.weight', 'context_interaction.layers.4.norm1.bias', 'context_interaction.layers.4.norm2.weight', 'context_interaction.layers.4.norm2.bias', 'context_interaction.layers.5.self_attn.in_proj_weight', 'context_interaction.layers.5.self_attn.in_proj_bias', 'context_interaction.layers.5.self_attn.out_proj.weight', 'context_interaction.layers.5.self_attn.out_proj.bias', 'context_interaction.layers.5.linear1.weight', 'context_interaction.layers.5.linear1.bias', 'context_interaction.layers.5.linear2.weight', 'context_interaction.layers.5.linear2.bias', 'context_interaction.layers.5.norm1.weight', 'context_interaction.layers.5.norm1.bias', 'context_interaction.layers.5.norm2.weight', 'context_interaction.layers.5.norm2.bias', 'personality_cls.weight', 'personality_cls.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DialogVAD.from_pretrained('bert-base-uncased').cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD_Lexicons = pd.read_csv('data/NRC-VAD-Lexicon-Aug2018Release/NRC-VAD-Lexicon.txt', sep='\\t')\n",
    "VAD_dict = {}\n",
    "for r in VAD_Lexicons.iterrows():\n",
    "    VAD_dict[r[1]['Word']] = [r[1]['Valence'], r[1]['Arousal'], r[1]['Dominance']]\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.6.0.dev0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [batchsize * dialog_length * max_uttr_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAD_tokenized_dict\n",
    "\n",
    "import json\n",
    "\n",
    "with open('VAD_tokenized_dict.json','a') as outfile:\n",
    "    json.dump(VAD_tokenized_dict,outfile,ensure_ascii=False)\n",
    "    outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3], [4, 5]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2],[3,4]]\n",
    "b = [[i+1 for i in j] for j in a]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4836</th>\n",
       "      <th>4837</th>\n",
       "      <th>4838</th>\n",
       "      <th>4839</th>\n",
       "      <th>4840</th>\n",
       "      <th>4841</th>\n",
       "      <th>4842</th>\n",
       "      <th>4843</th>\n",
       "      <th>4844</th>\n",
       "      <th>4845</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 4846 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7      8      9     ...   4836  \\\n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.714  0.760  ...  0.229   \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.296  0.538  ...  0.847   \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.547  0.519  ...  0.571   \n",
       "\n",
       "   4837  4838  4839  4840  4841   4842  4843   4844   4845  \n",
       "0   0.0   0.0   0.0   0.0   0.0  0.521   0.0  0.612  0.510  \n",
       "1   0.0   0.0   0.0   0.0   0.0  0.336   0.0  0.721  0.235  \n",
       "2   0.0   0.0   0.0   0.0   0.0  0.491   0.0  0.606  0.426  \n",
       "\n",
       "[3 rows x 4846 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('VAD_tokenized_dict.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "cnt = 0\n",
    "V = []\n",
    "A = []\n",
    "D = []\n",
    "for k,v in data.items():\n",
    "    V.append(v[0])\n",
    "    A.append(v[1])\n",
    "    D.append(v[2])\n",
    "\n",
    "df = pd.DataFrame([V,A,D])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoebe Sr.\n",
      "['Phoebe Sr.', \" So I guess you'd like to know how it all happened.\"]\n",
      "Phoebe\n",
      "['Phoebe', \" I-I mean I, well I think I can figure it out. I guess y'know I was born, and everyone started lying their asses off!\"]\n",
      "Phoebe Sr.\n",
      "['Phoebe Sr.', \" Noo! No! It wasn't like that I... Remember how I told you how Lily, Frank, and I we were, we were close. Well, we were, we were very close.\"]\n",
      "Phoebe\n",
      "['Phoebe', ' How close?']\n",
      "Phoebe Sr.\n",
      "['Phoebe Sr.', ' Well, the-the three of us we were, kind of umm, a couple.']\n",
      "Phoebe\n",
      "['Phoebe', \" I don't even know how that would work!\"]\n",
      "Phoebe Sr.\n",
      "['Phoebe Sr.', ' Well, we were...']\n",
      "Phoebe\n",
      "['Phoebe', \" (interrupting) I'm not asking!\"]\n",
      "Phoebe Sr.\n",
      "['Phoebe Sr.', \" Well, any how, some how I got pregnant, and, and I was scared. I was stupid and sellfish, and I was 18 years old. I mean, you remember what it's like to be eighteen years old?\"]\n",
      "Phoebe\n",
      "['Phoebe', \" Yeah. Let's see, my had Mom killed herself, and my Dad had run off, and I was living in a Gremlin with a guy named Cindy who talked to his hand.\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sents = [['Phoebe Sr.', \" So I guess you'd like to know how it all happened.\"], ['Phoebe', \" I-I mean I, well I think I can figure it out. I guess y'know I was born, and everyone started lying their asses off!\"], ['Phoebe Sr.', \" Noo! No! It wasn't like that I... Remember how I told you how Lily, Frank, and I we were, we were close. Well, we were, we were very close.\"], ['Phoebe', ' How close?'], ['Phoebe Sr.', ' Well, the-the three of us we were, kind of umm, a couple.'], ['Phoebe', \" I don't even know how that would work!\"], ['Phoebe Sr.', ' Well, we were...'], ['Phoebe', \" (interrupting) I'm not asking!\"], ['Phoebe Sr.', \" Well, any how, some how I got pregnant, and, and I was scared. I was stupid and sellfish, and I was 18 years old. I mean, you remember what it's like to be eighteen years old?\"], ['Phoebe', \" Yeah. Let's see, my had Mom killed herself, and my Dad had run off, and I was living in a Gremlin with a guy named Cindy who talked to his hand.\"]]\n",
    "ans = \"\"\n",
    "for i in sents:\n",
    "        print(i[0])\n",
    "        if i[0].split(' ')[0] != 'Phoebe' or i[0] == ['Phoebe Sr.']:\n",
    "            ans = ans + ' ' + i[1]\n",
    "        else:\n",
    "            print(i)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No Okay Why does everyone keep fixating on that She didnt know how should I know Sometimes I wish I was a lesbian They all stare at him Did I say that out loud I told mom and dad last night they seemed to take it pretty well Oh really so that hysterical phone call I got from a woman at sobbing 300 AM Ill never have grandchildren Ill never have grandchildren was what A wrong number Sorry Alright Ross look Youre feeling a lot of pain right now Youre angry Youre hurting Can I tell you what the answer is Strip joint Cmon youre single Have some hormones I dont want to be single okay I just I just I just wanna be married again\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "sent = df_o['sent'][0]\n",
    "for i in string.punctuation:\n",
    "    sent = sent.replace(i, \"\")\n",
    "print(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
